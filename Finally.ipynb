{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKWUgpUpAJrv"
   },
   "source": [
    "# Установка и импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIGKWvY5_4nE"
   },
   "outputs": [],
   "source": [
    "!pip install haversine\n",
    "!pip install catboost;\n",
    "!pip install ipywidgets;\n",
    "!jupyter nbextension enable --py widgetsnbextension;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8V1ndOLeADTr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMMIDKFYAJGG"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvH3aCSTAPbf"
   },
   "source": [
    "# Добавление открытых данных о метро\n",
    "\n",
    "На мой взгляд, метро оказывает сильное воздействие на недвижимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozs0Y5-iAaPL"
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_excel('/content/drive/My Drive/HackTheRealty/E/data/metro.xlsx')\n",
    "df = df[['NameOfStation_en', 'Longitude_WGS84_en', 'Latitude_WGS84_en']]\n",
    "df = df.reset_index().drop('index', axis=1)\n",
    "df = df.rename({'NameOfStation_en': 'metro_station', 'Longitude_WGS84_en': 'metro_longitude', 'Latitude_WGS84_en': 'metro_latitude'}, axis=1)\n",
    "\n",
    "sr = df_1['NameOfStation_en']\n",
    "sr[df_1['Name_en'] == 'Коммунарка, вход-выход 4'] = 'Kommunarka'\n",
    "sr[df_1['Name_en'] == 'Ольховая, вход-выход 1'] = 'Olkhovaya'\n",
    "sr[df_1['Name_en'] == 'Прокшино, вход-выход 2'] = 'Prokshino'\n",
    "sr[df_1['Name_en'] == 'Филатов луг, вход-выход 2'] = 'Filatov lug'\n",
    "sr[df_1['Name_en'] == 'Савёловская, вход-выход 2'] = 'Savelovskaya'\n",
    "sr[df_1['Name_en'] == 'Коптево, вход-выход 2'] = 'Koptevo'\n",
    "sr[df_1['Name_en'] == 'Панфиловская, вход-выход 1'] = 'Panfilovskaya'\n",
    "sr[df_1['Name_en'] == 'Зорге, вход-выход 3'] = 'Zorge'\n",
    "sr[df_1['Name_en'] == 'Беломорская, вход-выход 2'] = 'Belomorskaya'\n",
    "sr[df_1['Name_en'] == 'Говорово, вход-выход 1 в вестибюль 2'] = 'Govorovo'\n",
    "sr[df_1['Name_en'] == 'Озерная, вход-выход 1, вестибюль 2'] = 'Ozernaya'\n",
    "sr[df_1['Name_en'] == 'Шелепиха, вход-выход 4 во 2-й вестибюль'] = 'Shelepiha'\n",
    "sr[df_1['Name_en'] == 'ЦСКА, вход-выход 1 во 2-й вестибюль'] = 'CSKA'\n",
    "sr[df_1['Name_en'] == 'Петровский парк, вход-выход 3 во 2-й вестибюль'] = 'Petrovskii park' \n",
    "\n",
    "df_1['NameOfStation_en'] = sr\n",
    "\n",
    "df_1 = df_1.drop_duplicates(subset=['NameOfStation_en'], ignore_index=True)\n",
    "df = df_1[['NameOfStation_en', 'Longitude_WGS84_en', 'Latitude_WGS84_en']]\n",
    "df = df.rename({'NameOfStation_en': 'metro_station', 'Longitude_WGS84_en': 'metro_longitude', 'Latitude_WGS84_en': 'metro_latitude'}, axis=1)\n",
    "\n",
    "# df.to_csv('metro.csv', index=False)\n",
    "# files.download(\"metro.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lobv6_a9BHyn"
   },
   "source": [
    "# Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTFMAjYUBMHg"
   },
   "outputs": [],
   "source": [
    "def data_preproccesing(train_data):\n",
    "\n",
    "  train_data['renovation_int'] = train_data['renovation'].map({ 'UNKNOWN': 0, \n",
    "                                                      'COSMETIC_DONE': 1, \n",
    "                                                      'EURO': 2, \n",
    "                                                      'GOOD': 3,\n",
    "                                                      'NORMAL': 4,\n",
    "                                                      'DESIGNER_RENOVATION': 5,\n",
    "                                                      'NEEDS_RENOVATION': 6,\n",
    "                                                      'RENOVATED': 7,\n",
    "                                                      'PARTIAL_RENOVATION': 8,\n",
    "                                                      'COSMETIC_REQUIRED': 9,\n",
    "                                                      'PRIME_RENOVATION': 10}) \n",
    "  train_data = train_data.drop(['renovation'], axis=1)\n",
    "  train_data = train_data.rename({'renovation_int': 'renovation'}, axis=1)\n",
    "\n",
    "  train_data['balcony_int'] = train_data['balcony'].map({ 'UNKNOWN': 0, \n",
    "                                                      'BALCONY': 1, \n",
    "                                                      'LOGGIA': 2, \n",
    "                                                      'TWO_LOGGIA': 3,\n",
    "                                                      'TWO_BALCONY': 4,\n",
    "                                                      'BALCONY__LOGGIA': 5,\n",
    "                                                      'BALCONY__TWO_LOGGIA': 6,\n",
    "                                                      'THREE_LOGGIA': 7,\n",
    "                                                      'THREE_BALCONY': 8 }) \n",
    "  train_data = train_data.drop(['balcony'], axis=1)\n",
    "  train_data = train_data.rename({'balcony_int': 'balcony'}, axis=1)\n",
    "\n",
    "  train_data['building_type_int'] = train_data['building_type'].map({ 'PANEL': 0, \n",
    "                                                      'BRICK': 1, \n",
    "                                                      'MONOLIT': 2, \n",
    "                                                      'UNKNOWN': 3,\n",
    "                                                      'BLOCK': 4,\n",
    "                                                      'MONOLIT_BRICK': 5,\n",
    "                                                      'WOOD': 6 }) \n",
    "  train_data = train_data.drop(['building_type'], axis=1)\n",
    "  train_data = train_data.rename({'building_type_int': 'building_type'}, axis=1)\n",
    "\n",
    "  train_data['parking_int'] = train_data['parking'].map({ 'UNKNOWN': 0, \n",
    "                                                      'OPEN': 1, \n",
    "                                                      'UNDERGROUND': 2, \n",
    "                                                      'CLOSED': 3 }) \n",
    "  train_data = train_data.drop(['parking'], axis=1)\n",
    "  train_data = train_data.rename({'parking_int': 'parking'}, axis=1)\n",
    "\n",
    "  train_data['day'] = pd.to_datetime(train_data['day'])\n",
    "  train_data['timestamp'] = train_data['day'].values.astype(np.int64) // 10 ** 9\n",
    "  train_data = train_data.drop(['day'], axis=1)\n",
    "\n",
    "  train_data = train_data.set_index(['id'])\n",
    "  train_data['timestamp'] = train_data['timestamp'].astype(float)\n",
    "  train_data['price'] = train_data['price'].astype(float)\n",
    "  train_data['floor'] = train_data['floor'].astype(float)\n",
    "\n",
    "  train_data = train_data.drop(['studio', 'has_elevator', 'expect_demolition', 'is_apartment'], axis=1)\n",
    "  train_data = train_data.drop(['site_id'], axis=1)\n",
    "  train_data = train_data.drop(['total_area'], axis=1)\n",
    "  train_data = train_data.drop(['flats_count'], axis=1)\n",
    "  train_data = train_data.drop(['building_id'], axis=1)\n",
    "  train_data = train_data.drop(['floors_total'], axis=1)\n",
    "  train_data = train_data.drop(['kitchen_area'], axis=1)\n",
    "\n",
    "  train_data['build_year'] = train_data['build_year'].astype(float)\n",
    "  train_data = train_data.drop(['unified_address'], axis=1) \n",
    "\n",
    "  le = preprocessing.LabelEncoder()\n",
    "  train_data['locality_name'] = le.fit_transform(train_data['locality_name'].values)\n",
    "\n",
    "  ser = pd.Series(train_data['main_image'])\n",
    "  ser[ser.isnull()] = 0\n",
    "  ser[ser != 0] = 1\n",
    "  ser = ser.astype(int)\n",
    "  train_data['main_image'] = ser\n",
    "\n",
    "  return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6Lgbev3BzEi"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/content/drive/My Drive/HackTheRealty/E/data/exposition_train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('/content/drive/My Drive/HackTheRealty/E/data/exposition_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu0fs8PCB1Cw"
   },
   "outputs": [],
   "source": [
    "train_df = data_preproccesing(train_data)\n",
    "train_df = train_df.drop(['target_string'], axis=1)\n",
    "\n",
    "test_df = data_preproccesing(test_data)\n",
    "arr = []\n",
    "for name in train_df:\n",
    "  if name != 'target':\n",
    "    arr.append(name)\n",
    "test_df = test_df[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgiPaXSHB2tD"
   },
   "outputs": [],
   "source": [
    "metro = pd.read_csv('/content/drive/My Drive/HackTheRealty/E/data/metro.csv')\n",
    "\n",
    "def nearest_metro(house, df):\n",
    "  min_dist = 100\n",
    "  near_metro = ''\n",
    "  for i in range(len(df['metro_station'])):\n",
    "    metro = (df['metro_latitude'][i], df['metro_longitude'][i])\n",
    "    if haversine(house, metro) < min_dist:\n",
    "      min_dist = haversine(house, metro)\n",
    "      near_metro = df['metro_station'][i]\n",
    "  return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2b5jhL-B4KR"
   },
   "outputs": [],
   "source": [
    "metro_distances_test = []\n",
    "\n",
    "lat_test = np.array(test_df['latitude'])\n",
    "lon_test = np.array(test_df['longitude'])\n",
    "\n",
    "for i in tqdm_notebook(range(len(lat_test))):\n",
    "  house = (lat_test[i], lon_test[i])\n",
    "  metro_distances_test.append(nearest_metro(house, metro))\n",
    "\n",
    "metro_distances = []\n",
    "\n",
    "lat = np.array(train_df['latitude'])\n",
    "lon = np.array(train_df['longitude'])\n",
    "\n",
    "for i in tqdm_notebook(range(len(lat))):\n",
    "  house = (lat[i], lon[i])\n",
    "  metro_distances.append(nearest_metro(house, metro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gu012PfAB5tn"
   },
   "outputs": [],
   "source": [
    "test_df['metro'] = metro_distances_test\n",
    "train_df['metro'] = metro_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHdPspiFB7Vb"
   },
   "outputs": [],
   "source": [
    "# test_df.to_csv('test_df.csv', index=False)\n",
    "# files.download(\"test_df.csv\")\n",
    "\n",
    "# train_df.to_csv('train_df.csv', index=False)\n",
    "# files.download(\"train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbjvh2gFCK_x"
   },
   "outputs": [],
   "source": [
    "train_df['timestamp'] = train_df['timestamp'] / 10e9\n",
    "test_df['timestamp'] = test_df['timestamp'] / 10e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7bKnUsECcon"
   },
   "outputs": [],
   "source": [
    "train_df['is_living'] = 0\n",
    "train_df['is_living'][train_df['living_area'] != 0] = 1\n",
    "train_df = train_df.drop('living_area', axis=1)\n",
    "\n",
    "test_df['is_living'] = 0\n",
    "test_df['is_living'][test_df['living_area'] != 0] = 1\n",
    "test_df = test_df.drop(['living_area'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO0nEg1XCgfK"
   },
   "source": [
    "# Training CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzd0Kw3vClon"
   },
   "outputs": [],
   "source": [
    "X = train_df.drop('target', axis=1)\n",
    "y = train_df['target']\n",
    "\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "categorical_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jufVtehCnDb"
   },
   "outputs": [],
   "source": [
    "modelR = CatBoostRegressor(iterations=1000,\n",
    "                             learning_rate=0.02,\n",
    "                             depth=10,\n",
    "                             eval_metric='RMSE',\n",
    "                             random_seed = 1717,\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             od_wait=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MqJ49q2Coyz"
   },
   "outputs": [],
   "source": [
    "modelR.fit(X, y, cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgqjiU7dCqbf"
   },
   "outputs": [],
   "source": [
    "# modelR.save_model('modelR')\n",
    "# files.download('modelR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsVU7tlkCubr"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leWm7asyCrSR"
   },
   "outputs": [],
   "source": [
    "pred = np.around(modelR.predict(test_df)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DcMW3MgC9CV"
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('/content/drive/My Drive/HackTheRealty/E/data/exposition_sample_submission.tsv', sep='\\t')\n",
    "# submission['target'] = pred\n",
    "# submission.to_csv('submission_last.tsv', sep='\\t', index=False)\n",
    "# files.download(\"submission_last.tsv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finally.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
